{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Title: What's a life pro tip, anyways?\n",
    "Date: 2019-05-12 12:00\n",
    "Tags: python\n",
    "Slug: reddit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the difference between a life pro tip, and one that is a bit more questionable? This can be sometimes a subtle difference, or a moral grey area to distinguish, even for humans.  \n",
    "\n",
    "Life Pro Tip: \n",
    "> A concise and specific tip that improves life for you and those around you in a specific and significant way.\n",
    "\n",
    "<br>\n",
    "\n",
    "_Example_: \"If you want to learn a new language, figure out the 100 most frequently used words and start with them. Those words make up about 50% of everyday speech, and should be a very solid basis.\"\n",
    "\n",
    "> An Unethical Life Pro Tip is a tip that improves your life in a meaningful way, perhaps at the expense of others and/or with questionable legality. Due to their nature, do not actually follow any of these tips–they're just for fun. \n",
    "\n",
    "<br>\n",
    "\n",
    "Example: \"Save business cards of people you don't like. If you ever hit a parked car accidentally, just write \"sorry\" on the back and leave it on the windshield.\"\n",
    "\n",
    "Let's collect posts (web scrap) from 2 subreddits, and create a machine learning model using Natural Langauge Processing (NLP) to classify which subreddit a particular post belongs too. Can my model pick up on sarcasm, internet 'trolling', or tongue-in-cheek semantic of sentences? Probably not, but let's try. I hope you have as much fun playing with this, as I did making it. \n",
    "\n",
    "If you're feeling lucky, visit my app for a Life Pro Tip!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Reddit API \n",
    "\n",
    "Fortunately, Reddit provides a public JSON end point, so we can easily consume that format, and manipulate it a Pandas DataFrame. Simply add `.json` at the end of the URL.\n",
    "\n",
    "If you plan to run your own `get` requests, keep in mind that Reddit has a limit of 25 posts / request. In conjunction with `for` loop, write a `time.sleep()` function in Python (or something equivalent) to avoid a 429 Too Many Requests error. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data dictionary\n",
    "\n",
    "We are interested in the following features:\n",
    "\n",
    "<table class=\"table table-responsive table-bordered\">\n",
    "<thead>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td><b>Target variable, y </b></td>\n",
    "<td> <p> subreddit (str) </p> </td>\n",
    "</tr>\n",
    "    \n",
    "<tr>\n",
    "<td><b>Design matrix, X </b></td>\n",
    "<td> \n",
    "<p>title (str)</p>\n",
    "<p>score (int)</p>\n",
    "<p>num_comments (int)</p>\n",
    "<p>author (int)</p>\n",
    "<p>name (int)</p>\n",
    "</ul>\n",
    "</td></tr>   \n",
    "\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "# Pre-processing data\n",
    "\n",
    "\n",
    "First, I have to pre-process the data, and use natural language processing packages to tokenize strings to individual words. We will be using Python's Natural Language Toolkit (nltk) package.\n",
    "\n",
    "Follow along if you want to create your own classifier, otherwise, skip to the results. All code is available on GitHub. Refer to reddit_garry.py for scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, LabelBinarizer\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "import pickle\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n",
    "warnings.filterwarnings(action='ignore', category=FutureWarning) \n",
    "np.set_printoptions(suppress=True)\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# encoding utf-8 for special characters\n",
    "raw_lpt = pd.read_csv(\"./lpt.csv\", encoding='utf-8')\n",
    "raw_ulpt = pd.read_csv(\"./ulpt.csv\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge, train, test split data. \n",
    "\n",
    "Notice how there is \"ULPT\" or \"LPT\" in the title, which is clearly target leakage. To prevent target leakage in the title, I will use regular expressions (Regex) to match permutations of LPT, lpt, ULPT, ulpt and remove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe table table-responsive table-striped table-bordered\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>name</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1287</th>\n",
       "      <td>UnfairCorner</td>\n",
       "      <td>t3_a76prs</td>\n",
       "      <td>285</td>\n",
       "      <td>9174</td>\n",
       "      <td>UnethicalLifeProTips</td>\n",
       "      <td>ULPT: If you have two pets of the same breed and colour, you only have to get pet insurance for one.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>mnkymnk</td>\n",
       "      <td>t3_aet1r2</td>\n",
       "      <td>609</td>\n",
       "      <td>25580</td>\n",
       "      <td>LifeProTips</td>\n",
       "      <td>LPT: Take a videocamera and spend 10min filming every room and every item in your house. Upload footage to the cloud. If you are ever in the unfortunate situation of a house-fire, this will make the insurance claim thousand times easier.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.merge(raw_lpt, raw_ulpt, how='outer')\n",
    "HTML(df.sample(2).to_html(classes=\"table table-responsive table-striped table-bordered\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.subreddit\n",
    "X = df.drop([\"subreddit\",'name'],axis=1) # drop name, it's a unique identifier, not predictive\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_to_words(raw_post):\n",
    "    '''Returns a list of words ready for classification, by tokenizing,\n",
    "    removing punctuation, setting to lower case and removing stop words.'''\n",
    "    tokenizer = RegexpTokenizer(r'[a-z]+')\n",
    "    words = tokenizer.tokenize(raw_post.lower())\n",
    "    meaningful_words = [w for w in words if not w in set(stopwords.words('english'))]\n",
    "    return(\" \".join(meaningful_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train.loc[:,\"title_clean\"] = X_train[\"title\"].apply(lambda row : re.sub(r\"[uU]*[lL][pP][tT]\\s*:*\", '', row)).apply(lambda row: post_to_words(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gc/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py:543: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "X_test.loc[:,\"title_clean\"] = X_test[\"title\"].apply(lambda row : re.sub(r\"[uU]*[lL][pP][tT]\\s*:*\", '', row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.loc[:,\"title_clean\"] = X_test[\"title\"].apply(lambda row: post_to_words(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer\n",
    "\n",
    "Let's start simple. CountVectorizer is a bag of words model processes text by ignoring structure of a sentences and merely assesses the count of specific words, or word combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_vectorizer(vectorizer,X_train,X_test,y_train,y_test,stop=None):\n",
    "    '''Takes a vectorizer, fits the model, learns the vocabulary,\n",
    "    transforms data and returns the transformed matrices'''\n",
    "    # transform text\n",
    "    vect = vectorizer(stop_words=stop) \n",
    "    train_data_features = vect.fit_transform(X_train.title)\n",
    "    test_data_features = vect.transform(X_test.title)\n",
    "    le = LabelEncoder()\n",
    "    target_train = le.fit_transform(y_train)\n",
    "    target_test = le.transform(y_test)\n",
    "    \n",
    "    # transform non text\n",
    "    mapper = DataFrameMapper([\n",
    "    (\"author\", LabelBinarizer()),\n",
    "    ([\"num_comments\"], StandardScaler()),\n",
    "    ([\"score\"], StandardScaler())], df_out=True)\n",
    "    Z_train = mapper.fit_transform(X_train)\n",
    "    Z_test = mapper.transform(X_test)\n",
    "    print(f' Learned distinct training vocabulary is {train_data_features.shape[1]}')\n",
    "    print(f' Remember: 0 -> {le.classes_[0]}, 1 -> {le.classes_[1]}')\n",
    "    \n",
    "    # Baseline model\n",
    "    print(f' Baseline model that guessed all LPT -> {round(1-sum(target_test)/len(target_test),2)} accurate')\n",
    "    \n",
    "    # Combine both df columns together\n",
    "    a = pd.DataFrame(train_data_features.todense())\n",
    "    b = Z_train\n",
    "    c = pd.DataFrame(test_data_features.todense())\n",
    "    d = Z_test\n",
    "\n",
    "    # reset indices in order to merge\n",
    "    a = a.reset_index().drop(\"index\",axis=1)\n",
    "    b = b.reset_index().drop(\"index\",axis=1)\n",
    "    c = c.reset_index().drop(\"index\",axis=1)\n",
    "    d = d.reset_index().drop(\"index\",axis=1)\n",
    "    \n",
    "    Z_train = pd.merge(a,b, left_index=True, right_index=True)\n",
    "    Z_test = pd.merge(c,d, left_index=True, right_index=True)\n",
    "    return (Z_train, Z_test, target_train, target_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Classification\n",
    "\n",
    "With my data ready in array format, I can now apply binary classifiers. I'll try:\n",
    "\n",
    "<ul>\n",
    "<li>Logistic Regression</li>\n",
    "<li>Naive Bayes Multinomial</li>\n",
    "<li>Support Vector Machine</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Learned distinct training vocabulary is 5208\n",
      " Remember: 0 -> LifeProTips, 1 -> UnethicalLifeProTips\n",
      " Baseline model that guessed all LPT -> 0.5 accurate\n"
     ]
    }
   ],
   "source": [
    "my_tuple = my_vectorizer(CountVectorizer,X_train,X_test,y_train,y_test,stop='english')\n",
    "Z_train = my_tuple[0]\n",
    "Z_test = my_tuple[1]\n",
    "target_train = my_tuple[2]\n",
    "target_test = my_tuple[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def results(model):\n",
    "    '''Return a sample of 5 wrong predictions'''\n",
    "    model.fit(Z_train, target_train)\n",
    "    print(f' Training accuracy: {model.score(Z_train, target_train)}')\n",
    "    print(f' Test accuracy: {model.score(Z_test, target_test)}')\n",
    "    predictions = model.predict(Z_test)\n",
    "    predictions = np.where(predictions==0,\"LifeProTips\",\"UnethicalLifeProTips\")\n",
    "    proba = model.predict_proba(Z_test) \n",
    "    # proba[:,0] probability that it's 0\n",
    "    final = pd.DataFrame(list(zip(predictions, proba[:,0], y_test, X_test.title_clean, X_test.num_comments, X_test.score)), columns=['prediction', 'proba_lpt','label', 'title_clean', \"num_comments\", \"score\"])   \n",
    "    # final.to_csv(\"final.csv\",index=False) # export for my app\n",
    "    wrong = final[final.prediction!=final.label]\n",
    "    return HTML(wrong.sample(2).to_html(classes=\"table table-responsive table-striped table-bordered\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Training accuracy: 1.0\n",
      " Test accuracy: 0.99581589958159\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe table table-responsive table-striped table-bordered\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prediction</th>\n",
       "      <th>proba_lpt</th>\n",
       "      <th>label</th>\n",
       "      <th>title_clean</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>UnethicalLifeProTips</td>\n",
       "      <td>0.327096</td>\n",
       "      <td>LifeProTips</td>\n",
       "      <td>Hey All, While We Appreciate All The Pokémon Go Tips, Please Keep Them To /R/PokemonGO. Thank You!</td>\n",
       "      <td>0</td>\n",
       "      <td>12728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451</th>\n",
       "      <td>UnethicalLifeProTips</td>\n",
       "      <td>0.441197</td>\n",
       "      <td>LifeProTips</td>\n",
       "      <td>Just Because The Election Is Over, Does Not Mean That This Subreddit Will Be Accepting Politics or Politic Related Tips. We Will Still Not Accept Them. Keep Those Posts To Their Proper Subreddits. - Thank you.</td>\n",
       "      <td>0</td>\n",
       "      <td>15175</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results(LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(Z_train, target_train);\n",
    "\n",
    "# create a pipeline and serialize model to file (for my app)\n",
    "pipe = Pipeline([(\"model\", model)])\n",
    "pickle.dump(pipe, open(\"pipe.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the largest coefficients which correspond to num_comments (col 6452), score (col 6453), column 653 & 3218, and peak into the words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "# my_coef = pd.DataFrame(list(zip(Z_train.columns,abs(model.coef_[0]))),columns=[\"x\",\"coef\"]).sort_values(by=\"coef\",ascending=False).head()                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_coef = pd.DataFrame(list(zip(Z_train.columns,model.coef_[0])),columns=[\"x\",\"coef\"])\n",
    "large_coef = my_coef[(my_coef[\"x\"]==\"num_comments\") | (my_coef[\"x\"]==\"score\") | (my_coef[\"x\"]==653) | (my_coef[\"x\"]==3218)] \n",
    "HTML(large_coef.to_html(classes=\"table table-responsive table-striped table-bordered\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvect = CountVectorizer(stop_words='english') \n",
    "train_data_features = cvect.fit_transform(X_train.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cvect.get_feature_names()[3218])\n",
    "print(cvect.get_feature_names()[653])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f'If the number of comments increases by 1, the likelihood of being an Unethical Life Pro Tip is {np.exp(-4.26)} more likely.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f'If the score (upvotes - downvotes) increases by 1, the likelihood of being an Unethical Life Pro Tip is {np.exp(-1.89)} more likely.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is really performant off the bat with 99.9% training accuracy and 99.6% test accuracy. \n",
    "\n",
    "There's many more false positives than false negatives. One could argue, false positives are not as bad\n",
    "since you don't want to heed the advice of a bad tip, but if you miss a life pro tip, it's not as damaging. If we wanted to be more strict, we could tweak the threshold such that only predictions > 75% would be classified as UnethicalLifeProTip.\n",
    "\n",
    "\"Give the same perfume to your wife and your girlfriend. It could save your ass one day.\" 🙅🏻‍♂️ - _Not a Life Pro Tip_ but was predicted a _Pro Tip_ \n",
    "\n",
    "* The more 'popular' i.e. more comments and score, the great likelihood that it is unethical. Controversial posts tend to gain more popularity.\n",
    " \n",
    "* If your document includes words 'pay' or 'business', then the likelihood of being unethical is 3x more likely. There's probably a lot of unethical comments around payment and businesses!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Term Frequency Inverse Document Frequency (TF-IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to CountVectorizer, TF-IDF vectorizer tells us which words are most discriminating between documents.\n",
    "Words that occur often in one document but don't occur in many documents are important and contain a great deal of discriminating power. Note, TF-IDF figures are between [0,1]. The score is based on how often a word is compared in your document (spam) and other documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Learned distinct training vocabulary is 5208\n",
      " Remember: 0 -> LifeProTips, 1 -> UnethicalLifeProTips\n",
      " Baseline model that guessed all LPT -> 0.5 accurate\n"
     ]
    }
   ],
   "source": [
    "my_tuple = my_vectorizer(TfidfVectorizer,X_train,X_test,y_train,y_test,stop='english')\n",
    "Z_train = my_tuple[0]\n",
    "Z_test = my_tuple[1]\n",
    "target_train = my_tuple[2]\n",
    "target_test = my_tuple[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(Z_train, target_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Training accuracy: 0.9790648988136776\n",
      " Test accuracy: 0.895397489539749\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe table table-responsive table-striped table-bordered\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prediction</th>\n",
       "      <th>label</th>\n",
       "      <th>title</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>UnethicalLifeProTips</td>\n",
       "      <td>LifeProTips</td>\n",
       "      <td>LPT: When applying for jobs online, save a copy of the job responsibilities and requirements. This information is usually not available after they stop accepting applications and will be useful when preparing for the interviews.</td>\n",
       "      <td>143</td>\n",
       "      <td>12465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>UnethicalLifeProTips</td>\n",
       "      <td>LifeProTips</td>\n",
       "      <td>LPT: Save your PowerPoint presentations with a .pps extension instead of .ppt. They'll open directly in presentation mode and PowerPoint will close when the slideshow is over.</td>\n",
       "      <td>324</td>\n",
       "      <td>23122</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results(LogisticRegression())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test accuracy decreases to 85.1% (overfitting) which is lower than CountVectorizer. There are probably not as many discriminating words. Common words are helpful in distinguishing between the two classes. Words of high frequency that are predictive of one of the classes.\n",
    "\n",
    "It's not entire clear which words are the most influential, some words might indicate sarcasm. Overall, it's impressive that a logistic regression model is so powerful already, let's try a few more algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Naive Bayes Classifier\n",
    "The multinomial Naive Bayes classifier is appropriate for classification with discrete features (e.g., word counts for text classification), as the columns of X are all integer counts.\n",
    "\n",
    "Note, this classifier accepts only positive values so I have run the abs function on my scaled features. While I have the option to add a prior, I have opted to have Sklearn estimate from training data directly. I don't have a strong opinion if a particular post is in one subreddit over the other. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Training accuracy: 0.9993021632937893\n",
      " Test accuracy: 0.9121338912133892\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe table table-responsive table-striped table-bordered\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prediction</th>\n",
       "      <th>label</th>\n",
       "      <th>title</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>UnethicalLifeProTips</td>\n",
       "      <td>LifeProTips</td>\n",
       "      <td>LPT: If you're an impulse spender or find it too easy to drop money on something, translate the price of an item into the hours that you have worked to make that amount of money. It really puts in perspective the cash value of an item against the value of your time.</td>\n",
       "      <td>900</td>\n",
       "      <td>25095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>UnethicalLifeProTips</td>\n",
       "      <td>LifeProTips</td>\n",
       "      <td>LPT If you’re heading to a busy cafe or restaurant that doesn’t take bookings arrange meet your friends at 10 minutes to the hour (9:50am or 6:50pm) instead of on the hour. You’ll beat out everyone who arranged to meet on the hour and get seated much sooner.</td>\n",
       "      <td>649</td>\n",
       "      <td>27442</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MultinomialNB()\n",
    "Z_train = my_tuple[0]\n",
    "Z_test = my_tuple[1]\n",
    "target_train = my_tuple[2]\n",
    "target_test = my_tuple[3]\n",
    "\n",
    "Z_train.num_comments = abs(Z_train.num_comments)\n",
    "Z_train.score = abs(Z_train.score)\n",
    "model.fit(Z_train, target_train)\n",
    "\n",
    "print(f' Training accuracy: {model.score(Z_train, target_train)}')\n",
    "print(f' Test accuracy: {model.score(Z_test, target_test)}')\n",
    "\n",
    "predictions = model.predict(Z_test)\n",
    "predictions = np.where(predictions==0,\"LifeProTips\",\"UnethicalLifeProTips\")\n",
    "final = pd.DataFrame(list(zip(predictions, y_test, X_test.title, X_test.num_comments, X_test.score)), columns=['prediction', 'label', 'title', \"num_comments\", \"score\"])\n",
    "wrong = final[final.prediction!=final.label] \n",
    "HTML(wrong.sample(2).to_html(classes=\"table table-responsive table-striped table-bordered\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[206  35]\n",
      " [  7 230]]\n",
      "True Negatives: 206\n",
      "False Positives: 35\n",
      "False Negatives: 7\n",
      "True Positives: 230\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "predictions = model.predict(my_tuple[1])\n",
    "print(confusion_matrix(my_tuple[3], predictions))\n",
    "tn, fp, fn, tp = confusion_matrix(my_tuple[3], predictions).ravel()\n",
    "print(\"True Negatives: %s\" % tn)\n",
    "print(\"False Positives: %s\" % fp)\n",
    "print(\"False Negatives: %s\" % fn)\n",
    "print(\"True Positives: %s\" % tp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While Naive Bayes also has a high training accuracy, it is severely overfitting. In this case, there are more false negatives than false positives. It tended predict that certain posts were unethical life pro tips, when in fact they are!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Support Vector Machines\n",
    "\n",
    "- Exceptional perfomance\n",
    "- Effective in high-dimensional data\n",
    "- Low risk of overfitting, but a black box method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Learned distinct training vocabulary is 5208\n",
      " Remember: 0 -> LifeProTips, 1 -> UnethicalLifeProTips\n",
      " Baseline model that guessed all LPT -> 0.5 accurate\n",
      " Training accuracy: 0.7662247034193999\n",
      " Test accuracy: 0.7573221757322176\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe table table-responsive table-striped table-bordered\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prediction</th>\n",
       "      <th>label</th>\n",
       "      <th>title</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>UnethicalLifeProTips</td>\n",
       "      <td>LifeProTips</td>\n",
       "      <td>LPT Whenever you receive a greeting card with money in it for your birthday (or any other special day), always act like you don't see the money and read the card out loud first. After that, then thank them for the money. People really appreciate when you take the time to enjoy their greeting cards.</td>\n",
       "      <td>1077</td>\n",
       "      <td>25192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>UnethicalLifeProTips</td>\n",
       "      <td>LifeProTips</td>\n",
       "      <td>LPT: People want someone to tell them what to do in emergency situations. For example while performing CPR on someone don't say \"Someone call an ambulance\" instead talk to one person and ask him/her to call an ambulance directly.</td>\n",
       "      <td>706</td>\n",
       "      <td>20378</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_tuple = my_vectorizer(CountVectorizer,X_train,X_test,y_train,y_test,stop='english')\n",
    "Z_train = my_tuple[0]\n",
    "Z_test = my_tuple[1]\n",
    "target_train = my_tuple[2]\n",
    "target_test = my_tuple[3]\n",
    "\n",
    "results(svm.SVC())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9979064898813678\n",
      "{'C': 1, 'gamma': 'scale'}\n",
      "0.99581589958159\n"
     ]
    }
   ],
   "source": [
    "params = {'C': [1,3],'gamma': [\"scale\"]}\n",
    "\n",
    "grid_search = GridSearchCV(svm.SVC(), param_grid=params, cv=5)\n",
    "grid_search.fit(Z_train, target_train)\n",
    "\n",
    "print(grid_search.best_score_)\n",
    "print(grid_search.best_params_)\n",
    "print(grid_search.score(Z_test,target_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Training accuracy: 1.0\n",
      " Test accuracy: 0.99581589958159\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe table table-responsive table-striped table-bordered\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prediction</th>\n",
       "      <th>label</th>\n",
       "      <th>title</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>UnethicalLifeProTips</td>\n",
       "      <td>LifeProTips</td>\n",
       "      <td>Hey All, While We Appreciate All The Pokémon Go Tips, Please Keep Them To /R/PokemonGO. Thank You!</td>\n",
       "      <td>0</td>\n",
       "      <td>12728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451</th>\n",
       "      <td>UnethicalLifeProTips</td>\n",
       "      <td>LifeProTips</td>\n",
       "      <td>Just Because The Election Is Over, Does Not Mean That This Subreddit Will Be Accepting Politics or Politic Related Tips. We Will Still Not Accept Them. Keep Those Posts To Their Proper Subreddits. - Thank you.</td>\n",
       "      <td>0</td>\n",
       "      <td>15175</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results(svm.SVC(1,gamma=\"scale\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of the box, SVC is not performant. We have to tune hyperparameters to improve the accuracy. Recall, if C is large, we do not regularize much (larger budget that the margin can be violated), leading to a more perfect classifier of our training data. Of course, there will be a trade off in overfitting and greater error due to higher variance. A smaller gamma helps with lower bias, by trading off with higher variance. Gamma = \"scale\", which uses `n_features` * `X.var()` tends to work well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table class=\"table table-striped table-responsive table-bordered\">\n",
    "<thead>\n",
    "</thead>\n",
    "<tbody>\n",
    "    \n",
    "<tr>\n",
    "<td><b> Classification Model </b></td>\n",
    "<td><b> Training Accuracy % </b></td>\n",
    "<td><b> Test Accuracy % </b></td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td><b>Baseline </b></td>\n",
    "<td> 0.5 </td>\n",
    "<td> 0.5 </td>\n",
    "</tr>   \n",
    "\n",
    "\n",
    "<tr>\n",
    "<td><b>Logistic </b></td>\n",
    "<td> 0.999 </td>\n",
    "<td> 0.996</td>\n",
    "</tr>   \n",
    "\n",
    "<tr>\n",
    "<td><b>Naive Bayes </b></td>\n",
    "<td> 0.999 </td>\n",
    "<td> 0.912 </td>\n",
    "</tr>  \n",
    "\n",
    "<tr>\n",
    "<td><b>Support Vector Machines </b></td>\n",
    "<td> 0.998 </td>\n",
    "<td> 0.996 </td>\n",
    "</tr>   \n",
    "\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given these results, my selected production model will be the logistic regression model with TF-IDF as the vectorizer. The Logistic Model is the equally as performant as the SVM, while providing much more interpret-ability.\n",
    "\n",
    "In conclusion:\n",
    "\n",
    "* The more 'popular' i.e. more comments and score, the great likelihood that it is a unethical. Controversial posts tend to gain more popularity.\n",
    "\n",
    "* If your document includes words 'pay' or 'business', then the likelihood of being unethical is 3x more likely. There's probably a lot of unethical comments around payment and businesses!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Wrap up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I was able to create an app using Natural Language Processing to classify which subreddit a particular post belongs to.\n",
    "\n",
    "While this was a fun use case of NLP, this analysis is widely applicable other areas, such as politics in classifying fake news vs. real news, or for eCommerce, for sentiment analysis of user reviews (i.e. polarity classification - positive, negative of neutral). Further, many virtual assistants (Amazon Alexa, Google Assistant) use NLP to understand the human's question and provide the appropriate response.\n",
    "\n",
    "As you can imagine, there would be far greater consequences, if the prediction was a false-positive or false-negative and tuning the model to adjust these thresholds is critical.\n",
    "\n",
    "As a next step, I hope to investigate other NLP open source packages such as [Spacy](https://spacy.io/)!"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "nteract": {
   "version": "0.14.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
